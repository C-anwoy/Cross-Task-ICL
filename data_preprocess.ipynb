{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eshaan/Cross-task-ICL/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('path_to_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import Prompt\n",
    "prompter=Prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def create_embedding(data,model):\n",
    "    o=model.encode(data, convert_to_tensor=True).detach().cpu()\n",
    "    return o\n",
    "\n",
    "def create_embeddings(dataset,prompter,id,model,print_example=False,batch_size=1024):\n",
    "    \n",
    "    num_batch=math.ceil(len(dataset)/batch_size)\n",
    "    \n",
    "    for batch_i in tqdm(range(num_batch),desc='Getting emb of batches'):\n",
    "        \n",
    "        batch_dataset=dataset[batch_i*batch_size:(batch_i+1)*batch_size]        \n",
    "        batch_inputs=[prompter.get_input_data_without_def(d,id) for d in batch_dataset]\n",
    "        emb=create_embedding(batch_inputs,model)\n",
    "        for i,d in enumerate(batch_dataset):\n",
    "            d['emb']=emb[i]\n",
    "            if print_example:\n",
    "                print(d)\n",
    "                print_example=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path='data/source'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "path='data/target'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "path='data/target-unlabeled'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split='data/source'\n",
    "k=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Fri Aug 11 17:12:00 2023) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "dataset_name='mnli'\n",
    "dataset= load_dataset('glue',dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s1=[]\n",
    "test_s2=[]\n",
    "test_l=[]\n",
    "import random\n",
    "label2text={\n",
    "    2:'contradiction',\n",
    "    1:'neutral',\n",
    "    0:'entailment'\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "        test_id.append(d['idx'])\n",
    "        test_s1.append(d['premise'])\n",
    "        test_s2.append(d['hypothesis'])\n",
    "        test_l.append(label2text[d['label']])\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence1':test_s1,\n",
    "    'sentence2':test_s2\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_df=test_df.sample(k,random_state=0)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt_temp_id='001'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'001',\n",
    "        'data':test_s_df,\n",
    "        'set':'source',\n",
    "        'instruction':' Given Sentence 1 which is a premise and Sentence 2 which is a hypothesis do natural language inference on the pair. In natural language inference we mark whether the premise and hypothesis are \"neutral\", \"contradiction\" or \"entailment\". The pair are said to be \"entailed\" if the premise justifies/supports the hypothesis, if the pair contradict each other we label them as \"contradiction\" and label them \"neutral\" in all other cases'\n",
    "}\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)\n",
    "with open(f'{split}/{dataset_name}_train.pkl','rb') as f:\n",
    "        test_set=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 348280,\n",
       " 'label': 'contradiction',\n",
       " 'sentence1': 'Before any legal action is brought against this rule in federal court, the administrative appeal rights set forth at 7 C.F.R.',\n",
       " 'sentence2': 'Before we send this to federal court, there is a conference.',\n",
       " 'emb': tensor([-2.5739e-02,  3.1649e-02,  3.0162e-03,  3.0356e-02,  1.2583e-02,\n",
       "          1.1141e-01,  1.2851e-02,  4.5885e-03,  3.2056e-02,  8.5702e-02,\n",
       "          3.3429e-02,  8.6360e-02, -4.3918e-02,  7.0449e-03,  4.8251e-02,\n",
       "         -6.9651e-03,  6.3258e-02, -1.4582e-02, -3.4724e-02,  3.6546e-02,\n",
       "          2.1035e-02,  1.0543e-02, -2.8848e-02,  2.6292e-02, -2.5695e-02,\n",
       "         -6.1129e-02, -1.0759e-01, -4.9241e-02,  5.7713e-02, -2.5417e-02,\n",
       "         -2.5361e-02,  1.2542e-02,  1.1923e-01,  5.1684e-02,  3.3623e-02,\n",
       "         -2.4713e-03,  4.2015e-02, -1.8845e-02,  3.5000e-02,  4.7962e-02,\n",
       "         -2.8507e-02, -7.7633e-02, -1.7560e-02, -3.3662e-02, -4.7150e-02,\n",
       "          7.9259e-02,  2.2829e-03,  2.2897e-02, -1.6318e-02,  4.9913e-02,\n",
       "         -3.8245e-05,  3.6166e-02, -7.9135e-03,  7.5692e-02,  2.3773e-02,\n",
       "          2.5033e-02,  1.1817e-02, -5.1543e-02,  3.5512e-02,  1.2777e-02,\n",
       "         -2.0610e-02, -1.1193e-01, -7.2470e-02,  4.8699e-02,  2.1145e-02,\n",
       "          7.5136e-03, -8.6330e-03, -4.6745e-02,  1.2955e-02, -3.8914e-03,\n",
       "          6.9924e-02,  5.8956e-02,  5.9714e-03,  6.1332e-02,  3.1551e-02,\n",
       "          4.0441e-02,  2.3825e-03,  3.5846e-02,  2.8050e-02, -7.7985e-02,\n",
       "          4.3935e-02, -1.4911e-02,  1.1744e-03, -9.7185e-02, -2.5844e-02,\n",
       "          3.8091e-02, -3.9965e-02,  1.3627e-03, -3.1971e-02,  3.3187e-02,\n",
       "          2.2203e-02, -6.1810e-02,  3.3556e-02, -1.0791e-02, -4.4358e-02,\n",
       "         -4.6436e-02, -4.8143e-02, -9.6617e-02,  6.2518e-02,  5.2142e-02,\n",
       "          2.0164e-02,  6.5626e-02, -4.5356e-02, -6.2631e-02,  4.3683e-02,\n",
       "         -7.4012e-02,  3.0269e-02, -9.0986e-03,  2.6030e-02,  4.1404e-03,\n",
       "         -1.9296e-02, -2.3615e-02,  7.1026e-02, -1.3662e-02, -4.6896e-02,\n",
       "          3.6793e-03,  5.1288e-02,  6.3124e-03,  1.8814e-02, -1.3707e-01,\n",
       "          3.6513e-02,  7.4275e-02, -1.0754e-03,  1.0232e-02, -1.0904e-02,\n",
       "         -2.6601e-02, -2.8332e-02, -4.7526e-35, -5.5874e-02, -8.3364e-02,\n",
       "          7.0519e-03, -5.0962e-02,  5.8050e-02, -7.9658e-02, -6.7257e-02,\n",
       "         -1.0154e-01,  2.9870e-02, -2.2027e-02, -4.2799e-03,  1.0188e-02,\n",
       "          9.1517e-02, -1.0598e-01, -2.4207e-02, -2.2073e-02, -7.3472e-02,\n",
       "         -6.8881e-03,  7.1868e-02,  6.0676e-03,  5.2749e-02, -3.6776e-02,\n",
       "         -5.1624e-02,  5.6676e-02, -1.0274e-01, -2.1905e-02, -4.8086e-02,\n",
       "         -1.5736e-02,  2.7534e-02, -1.6866e-03, -6.3080e-02,  7.6911e-02,\n",
       "          4.7725e-02,  3.8485e-02,  3.6913e-02,  2.7380e-02,  4.2953e-02,\n",
       "         -3.9598e-02,  4.9246e-02,  1.5734e-03, -2.9455e-02,  2.1322e-02,\n",
       "         -2.2956e-02, -5.5457e-02,  5.8754e-02, -4.6983e-02, -5.7232e-02,\n",
       "          5.2622e-04,  3.0770e-02,  5.7890e-02,  8.4109e-02,  1.6943e-03,\n",
       "          5.8314e-02, -2.3937e-02,  4.8200e-02, -2.1340e-02, -8.0277e-02,\n",
       "         -2.6099e-02,  1.4946e-02,  3.9246e-02,  5.9369e-02,  4.5606e-02,\n",
       "         -3.2495e-02,  3.0129e-02, -1.2720e-01, -1.9201e-02, -4.8537e-02,\n",
       "         -6.5539e-02, -8.5350e-03, -1.2192e-01,  6.9371e-03,  4.3625e-02,\n",
       "         -9.5543e-02,  3.0265e-02, -1.7316e-02, -4.5260e-02, -3.7987e-02,\n",
       "          4.8306e-02,  3.8912e-02, -6.0621e-02, -6.7229e-02, -1.0669e-02,\n",
       "          1.6387e-02,  3.3816e-02,  7.4308e-02, -4.0948e-02, -4.9342e-02,\n",
       "         -1.5309e-02, -7.5990e-02, -5.4918e-02, -4.9848e-02, -5.0903e-03,\n",
       "          1.9483e-02,  1.5058e-01,  7.1312e-02, -2.7264e-33,  2.5366e-02,\n",
       "         -6.5566e-02, -7.7411e-02,  5.6926e-02, -3.9841e-02,  5.2771e-02,\n",
       "         -2.0930e-02, -4.8276e-02, -3.8357e-02, -4.7017e-02, -2.6248e-02,\n",
       "         -2.1208e-03,  4.5617e-03,  4.7153e-02,  1.3005e-03, -5.0526e-02,\n",
       "          1.1752e-02, -1.5870e-02, -8.6415e-03,  3.0616e-02,  1.4100e-02,\n",
       "         -1.4040e-02, -3.3943e-02,  8.3336e-02, -2.2753e-02,  8.5612e-03,\n",
       "          1.0079e-02, -6.6836e-02,  3.3417e-02,  3.4519e-02, -1.1110e-01,\n",
       "         -6.7540e-02, -8.3693e-02,  6.9255e-02,  4.0813e-02, -5.2865e-02,\n",
       "          1.0358e-01, -6.6582e-03, -4.7241e-02, -8.6415e-03,  3.4029e-02,\n",
       "          4.2986e-02, -1.2326e-02, -2.0065e-02,  3.6153e-02,  1.1251e-02,\n",
       "          3.9300e-02,  3.8988e-02, -8.5015e-02, -1.3099e-02, -4.3639e-02,\n",
       "         -4.9454e-02,  1.1524e-01, -1.9799e-02, -5.1634e-02,  1.8965e-02,\n",
       "          6.2386e-02, -6.1074e-02,  2.0249e-02,  6.8624e-02,  1.2061e-01,\n",
       "          5.8239e-02, -8.5692e-02,  1.2502e-02,  4.8501e-02,  4.1838e-02,\n",
       "         -4.8070e-02,  1.2209e-01,  4.9380e-03, -5.4990e-02,  8.2194e-02,\n",
       "         -6.3461e-02, -1.5509e-01, -4.3902e-02,  1.3371e-02,  6.2137e-02,\n",
       "          8.0121e-02,  4.1509e-04, -8.0137e-02,  4.4671e-02, -5.4767e-02,\n",
       "          2.4596e-02, -4.9417e-02,  4.1486e-02,  8.1971e-03,  5.4944e-02,\n",
       "          2.3983e-02, -1.3823e-01, -3.3667e-02,  6.1593e-02, -6.6362e-02,\n",
       "         -1.2016e-02,  6.2358e-02,  2.6223e-02, -4.9460e-02, -4.0855e-08,\n",
       "         -7.5090e-03,  1.0944e-02,  1.2938e-02,  4.3513e-02,  1.6346e-02,\n",
       "         -1.3660e-02,  1.2331e-02, -1.1756e-01, -3.5084e-02, -6.5681e-02,\n",
       "          5.7427e-02,  6.9524e-02,  2.8959e-02,  2.8373e-02, -1.0538e-02,\n",
       "         -1.5615e-02, -3.6235e-02, -3.5667e-02, -2.3719e-02,  3.7060e-02,\n",
       "         -6.0841e-02, -7.1559e-02,  8.0032e-02, -6.2630e-02,  9.0501e-03,\n",
       "          3.3687e-02,  4.8896e-02,  6.9164e-02, -3.5976e-02,  9.1566e-02,\n",
       "         -5.3704e-02,  5.4023e-02, -7.4728e-02,  6.2425e-03,  3.0234e-02,\n",
       "         -2.1732e-02, -5.7666e-02, -1.2420e-02,  6.7092e-02, -7.5718e-02,\n",
       "         -6.3374e-02,  2.5408e-02,  3.1913e-02, -8.0814e-03,  3.3845e-02,\n",
       "          2.6512e-02, -4.6362e-02,  8.3631e-03,  3.3851e-02,  2.1017e-03,\n",
       "          3.8527e-03,  5.6878e-03,  8.6452e-03, -4.9843e-02,  1.1439e-01,\n",
       "          4.1138e-02,  4.4024e-02, -6.6492e-03, -8.8168e-03, -5.1109e-02,\n",
       "          5.8537e-02,  7.2131e-02,  5.1771e-02,  5.0906e-02])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Fri Aug 11 17:12:00 2023) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "dataset_name='qqp'\n",
    "dataset= load_dataset('glue',dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s1=[]\n",
    "test_s2=[]\n",
    "test_l=[]\n",
    "import random\n",
    "label2text={\n",
    "    1:'duplicate',\n",
    "    0:'not duplicate'\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "        test_id.append(d['idx'])\n",
    "        test_s1.append(d['question1'])\n",
    "        test_s2.append(d['question2'])\n",
    "        test_l.append(label2text[d['label']])\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence1':test_s1,\n",
    "    'sentence2':test_s2\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_df=test_df.sample(k,random_state=0)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 10/10 [00:05<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_temp_id='002'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'002',\n",
    "        'data':test_s_df,\n",
    "        'instruction':'Given two question pairs do text classification based on whether they are duplicates or not. The questions are mined from the popular online discussion forum Quora. As duplicate quetion might be present on Quora, the task is to label two identical questions as \"duplicate\" if they ask the same query else label the pair as \"not duplicate\".',\n",
    "        'set':'source'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/boolq/bf0dd57da941c50de94ae3ce3cef7fea48c08f337a4b7aac484e9dddc5aa24e5 (last modified on Fri Aug 11 17:16:37 2023) since it couldn't be found locally at boolq., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9427\n"
     ]
    }
   ],
   "source": [
    "dataset_name='boolq'\n",
    "dataset= load_dataset(dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_c=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "import random\n",
    "\n",
    "\n",
    "for d in dataset:\n",
    "        test_id.append(id)\n",
    "        id+=1\n",
    "        test_s.append(d['question'])\n",
    "        test_c.append(d['passage'])\n",
    "        test_l.append(str(d['answer']))\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s,\n",
    "    'context':test_c\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 10/10 [00:09<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_temp_id='003'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'003',\n",
    "        'data':test_s_df,\n",
    "        'instruction':'Given a context and a question do binary true and false type text classification. You are given a passage as context and a question related to the passage that can be answered as \"True\" or \"False\". Based on the context, question and your reasoning ability answer in a \"True\" and \"False\".',\n",
    "        'set':'source'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colll_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/conll2003/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98 (last modified on Fri Aug 11 17:27:41 2023) since it couldn't be found locally at conll2003., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "dataset_name='conll2003'\n",
    "dataset= load_dataset(dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "import random\n",
    "\n",
    "label2text={0:'O', \n",
    "1:'B-PER', \n",
    "2:'I-PER',\n",
    "3:'B-ORG',\n",
    "4:'I-ORG',\n",
    "5:'B-LOC',\n",
    "6:'I-LOC',\n",
    "7:'B-MISC',\n",
    "8:'I-MISC'\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "\n",
    "    l=[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "    test_id.append(d['id'])\n",
    "    for i in d['ner_tags']:\n",
    "        l.append(label2text[i])\n",
    "    test_s.append(' '.join(d['tokens']))\n",
    "    test_l.append(' '.join(l))\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_df=test_df.sample(k,random_state=0)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 10/10 [00:05<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_temp_id='004'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'004',\n",
    "        'data':test_s_df,\n",
    "        'instruction':'Given a sentence do token classification on it seek to locate and classify named entities mentioned in the sentence provided. The pre-defined named entity categories along with there labeles are Person (PER), Location (LOC), Organization (ORG) and Miscellaneous (MIS). If the token is not an entity mark it as None. As the entity is more than two tokens long use the prefix B with the named entity token to represent the beginning and  use the prefix I till the entity ends.',\n",
    "        'set':'source'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_ner_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conll-POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/conll2003/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98 (last modified on Fri Aug 11 17:27:41 2023) since it couldn't be found locally at conll2003., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "dataset_name='conll2003'\n",
    "dataset= load_dataset(dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "import random\n",
    "\n",
    "label={'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,\n",
    " 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,\n",
    " 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,\n",
    " 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,\n",
    " 'WP': 44, 'WP$': 45, 'WRB': 46}\n",
    "\n",
    "\n",
    "label2text=dict([(v,k) for k,v in label.items()])\n",
    "\n",
    "\n",
    "for d in dataset:\n",
    "\n",
    "    l=[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "    test_id.append(d['id'])\n",
    "    for i in d['pos_tags']:\n",
    "        l.append(label2text[i])\n",
    "    test_s.append(' '.join(d['tokens']))\n",
    "    test_l.append(' '.join(l))\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_df=test_df.sample(k,random_state=0)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 10/10 [00:05<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_temp_id='004'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'004',\n",
    "        'data':test_s_df,\n",
    "        'instruction':'Given a sentence do token classification by doing Part-of-speech (POS) tagging, which is a process in natural language processing (NLP) where each word in a text is labeled with its corresponding part of speech. This can include nouns, verbs, adjectives, and other grammatical categories.',\n",
    "        'set':'source'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_pos_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonsense_QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/commonsense_qa/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b (last modified on Mon Sep 11 14:41:55 2023) since it couldn't be found locally at commonsense_qa., or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "dataset_name='commonsense_qa'\n",
    "dataset= load_dataset('commonsense_qa')['validation']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "\n",
    "text2label={\n",
    "    'A':0,\n",
    "    'B':1,\n",
    "    'C':2,\n",
    "    'D':3,\n",
    "    'E':4\n",
    "}\n",
    "\n",
    "label2text={\n",
    "    0:'\\nA. ',\n",
    "    1:'\\nB. ',\n",
    "    2:'\\nC. ',\n",
    "    3:'\\nD. ',\n",
    "    4:'\\nE. '\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "    test_id.append(d['id'])\n",
    "    test_l.append(d['answerKey'])\n",
    "\n",
    "    q=d['question']\n",
    "    for i,a in enumerate(d['choices']['text']):\n",
    "        q+=' '+label2text[i]+a\n",
    "\n",
    "    test_s.append(q)\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 2/2 [00:00<00:00,  2.85it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_temp_id='005'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'005',\n",
    "        'data':test_s_df,\n",
    "        'instruction':'The following task relates to commonsense reasoning. It consists of a question that can be easily solved using logical abilities and reasoning, a set of five options  \"A.\", \"B.\", \"C.\", \"D.\" and \"E.\" are also provided along with the question, one of these options answers the question logically. Use your reasoning ability to select the most appropriate answer from the provided choices \"A.\", \"B.\", \"C.\", \"D.\" and \"E.\" and assign these choices (i.e  \"A.\", \"B.\", \"C.\", \"D.\" and \"E.\") as the label',  \n",
    "        'set':'source'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARC-Easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/ai2_arc/1569c2591ea2683779581d9fb467203d9aa95543bb9b75dcfde5da92529fd7f6 (last modified on Fri Aug 11 17:17:10 2023) since it couldn't be found locally at ai2_arc., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2241\n"
     ]
    }
   ],
   "source": [
    "dataset_name='ARC-Easy'\n",
    "dataset= load_dataset('ai2_arc',dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "import random\n",
    "\n",
    "label2text={\n",
    "    'A':'A',\n",
    "    'B':'B',\n",
    "    \"C\":\"C\",\n",
    "    \"D\":\"D\",\n",
    "    '1':'A',\n",
    "    '2':'B',\n",
    "    \"3\":\"C\",\n",
    "    \"4\":\"D\",\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "    if len(d['choices']['text'])!=4:\n",
    "        continue\n",
    "    test_id.append(d['id'])\n",
    "    id+=1\n",
    "    test_s.append(d['question']+'\\nA. '+d['choices']['text'][0]+'\\nB. '+d['choices']['text'][1]+'\\nC. '+d['choices']['text'][2]+'\\nD. '+d['choices']['text'][3])\n",
    "    test_l.append(label2text[d['answerKey']])\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_temp_id='005'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'005',\n",
    "        'data':test_s_df,\n",
    "        'instruction':'Given a question answering task from the 3rd to 9th-grade science exam. The question contains four options \"A.\", \"B.\", \"C.\" and \"D.\" Select the most appropriate choice that answers the question',\n",
    "        'set':'source'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/race/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b (last modified on Fri Aug 11 17:33:56 2023) since it couldn't be found locally at race., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "dataset_name='race'\n",
    "dataset= load_dataset('race','all')['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_c=[]\n",
    "test_l=[]\n",
    "import random\n",
    "\n",
    "\n",
    "for d in dataset:\n",
    "\n",
    "    q=d['question']+' \\nA. '+d['options'][0]+' \\nB. '+d['options'][1]+' \\nC. '+d['options'][2]+' \\nD. '+d['options'][3]\n",
    "\n",
    "    test_id.append(d['example_id'])\n",
    "    test_s.append(q)\n",
    "    test_c.append(d['article'])\n",
    "    test_l.append(d['answer'])\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s,\n",
    "    'context':test_c\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_df=test_df.sample(k,random_state=0)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 10/10 [00:16<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt_temp_id='003'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'003',\n",
    "        'data':test_s_df,\n",
    "        'instruction':'Given a reading comprehension type question-answering from an english exam for school students. You are given a context and multiple choice question containing four options \"A.\", \"B.\", \"C.\" and \"D.\". The question is answerable from the comprehension. Based on the question, the option and the context select the most appropriate answer from the provided choices \"A.\", \"B.\", \"C.\" and \"D.\".',                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "        'set':'source'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AG_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/ag_news/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548 (last modified on Fri Aug 11 17:28:02 2023) since it couldn't be found locally at ag_news., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "dataset_name='ag_news'\n",
    "dataset= load_dataset(dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "import random\n",
    "\n",
    "label2text={\n",
    "        0:'world',\n",
    "        1:'sports',\n",
    "        2:'business',\n",
    "        3:'technology'\n",
    "}\n",
    "\n",
    "\n",
    "for d in dataset:\n",
    "\n",
    "    test_id.append(id)\n",
    "    id+=1\n",
    "    test_s.append(d['text'])\n",
    "    test_l.append(label2text[d['label']])\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_df=test_df.sample(k,random_state=0)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 10/10 [00:06<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_temp_id='004'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'004',\n",
    "        'data':test_s_df,\n",
    "        'instruction':'Given a sentence do text classification, the sentence is a clipping from a news article that may be either related to sports, business, technology, or world news. You are to recognize the category of the sentence and label them as \"sports\", \"business\", \"technology\" or \"world\" news',\n",
    "        'set':'source'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Fri Aug 11 17:12:00 2023) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "dataset_name='sst2'\n",
    "dataset= load_dataset('glue','sst2')['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "import random\n",
    "label2text={\n",
    "    1:'positive',\n",
    "    0:'negative'\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "        test_id.append(d['idx'])\n",
    "        test_s.append(d['sentence'])\n",
    "        test_l.append(label2text[d['label']])\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_df=test_df.sample(k,random_state=0)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 10/10 [00:05<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_temp_id='004'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'004',\n",
    "        'data':test_s_df,\n",
    "        'instruction':'Given a movie review do text classification, based on the sentiment conveyed by the review label it as \"positive\" or \"negative\"',\n",
    "        'set':'source'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=500\n",
    "split='data/target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataset(data_list, N = k):\n",
    "    \n",
    "    selected_samples = []\n",
    "\n",
    "    if type(data_list[0]['label']) == list:\n",
    "        labels = list(set([example['label'][0] for example in data_list]))\n",
    "\n",
    "        print(f\"Label Space: {labels}\")\n",
    "        count = {}\n",
    "        for label in labels:\n",
    "            count[label] = 0\n",
    "\n",
    "        np.random.seed(0)\n",
    "        chosen_indices = np.random.randint(low = 0, high = len(data_list), size = len(data_list))\n",
    "    \n",
    "        for chosen in chosen_indices:\n",
    "            if (chosen not in selected_samples) and (count[data_list[chosen]['label'][0]] < (N/(len(count)))):\n",
    "                selected_samples.append(chosen)\n",
    "                count[data_list[chosen]['label'][0]] +=1\n",
    "\n",
    "        selected_samples.sort()\n",
    "\n",
    "        print(f\"Count of selected labels: {count}\")\n",
    "\n",
    "        selected_data = list(np.array(data_list)[selected_samples])\n",
    "        return selected_data, labels\n",
    "\n",
    "    else:\n",
    "        labels = list(set([example['label'] for example in data_list]))\n",
    "\n",
    "        print(f\"Label Space: {labels}\")\n",
    "        count = {}\n",
    "        for label in labels:\n",
    "            count[label] = 0\n",
    "\n",
    "        np.random.seed(0)\n",
    "        chosen_indices = np.random.randint(low = 0, high = len(data_list), size = len(data_list))\n",
    "\n",
    "        for chosen in chosen_indices:\n",
    "\n",
    "            if (chosen not in selected_samples) and (count[data_list[chosen]['label']] < (N/(len(count)))):\n",
    "                selected_samples.append(chosen)\n",
    "                count[data_list[chosen]['label']] +=1\n",
    "\n",
    "\n",
    "        selected_samples.sort()\n",
    "        print(f\"Count of selected labels: {count}\")\n",
    "        selected_data = list(np.array(data_list)[selected_samples])\n",
    "        return selected_data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Med MCQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/medmcqa/f2fdfa9ccfbf9d148c0639e6afe3379f3c7e95c4d52d5e68ec1156e5004bd880 (last modified on Thu Sep 14 11:39:40 2023) since it couldn't be found locally at medmcqa., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4183\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'medmcqa'\n",
    "dataset= load_dataset(dataset_name)['validation']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "\n",
    "text2label={\n",
    "    0:'A',\n",
    "    1:'B',\n",
    "    2:'C',\n",
    "    3:'D'\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "    test_id.append(d['id'])\n",
    "    test_l.append(text2label[d['cop']])\n",
    "\n",
    "    q=d['question']+' \\nA. '+d['opa']+' \\nB. '+d['opb']+' \\nC. '+d['opc']+' \\nD. '+d['opd']\n",
    "\n",
    "    test_s.append(q)\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Space: ['C', 'D', 'A', 'B']\n",
      "Count of selected labels: {'C': 125, 'D': 125, 'A': 125, 'B': 125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_data, labels = create_new_dataset(test_s_df)\n",
    "prompt_temp_id='005'\n",
    "create_embeddings(selected_data,prompter,prompt_temp_id,model)\n",
    "data_to_save={\n",
    "          'dataset_name': dataset_name,\n",
    "          'prompt_temp_id':'005',\n",
    "          'data': selected_data,\n",
    "          'instruction': 'Given a multiple choice question containing four options \"A.\", \"B.\", \"C.\" and \"D.\" from a medical entrance exam. The question is related to a sub-field of medical science like Microbiology, Radiology, Ophthalmology, Surgery, Human anatomy, etc. Based on the question, the option and your knowledge of the medical field select the most appropriate answer from the provided choices \"A.\", \"B.\", \"C.\" and \"D.\".',\n",
    "          'labels': labels,\n",
    "          'set':'target'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_test.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/sciq/50e5c6e3795b55463819d399ec417bfd4c3c621105e00295ddb5f3633d708493 (last modified on Thu Sep 14 11:41:12 2023) since it couldn't be found locally at sciq., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'sciq'\n",
    "dataset= load_dataset(dataset_name)['test']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "import random\n",
    "text2label={\n",
    "    'A':'distractor3',\n",
    "    'B':'distractor1',\n",
    "    'C':'distractor2',\n",
    "    'D':'correct_answer'\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "\n",
    "    k=['A','B','C','D']\n",
    "    k_=['A','B','C','D']\n",
    "    random.shuffle(k)\n",
    "    q=d['question']   \n",
    "    for i,l in zip(k,k_):\n",
    "        op=text2label[i]\n",
    "        if op=='correct_answer':\n",
    "            test_l.append(l)\n",
    "\n",
    "        q+=f'\\n{l}. '+d[op]\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "    test_id.append(id)\n",
    "    id+=1\n",
    "    test_s.append(q)\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Space: ['C', 'D', 'A', 'B']\n",
      "Count of selected labels: {'C': 125, 'D': 125, 'A': 125, 'B': 125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_data, labels = create_new_dataset(test_s_df)\n",
    "prompt_temp_id='005'\n",
    "create_embeddings(selected_data,prompter,prompt_temp_id,model)\n",
    "data_to_save={\n",
    "          'dataset_name': dataset_name,\n",
    "          'prompt_temp_id':'005',\n",
    "          'data': selected_data,\n",
    "          'instruction': 'Given a question from a scientific exam about Physics, Chemistry, and Biology, among others. The question is in multiple choice format with four answer options \"A.\", \"B.\", \"C.\" and \"D.\". Using your knowledge about the scientific fields answer the question and provide the label \"A\", \"B\", \"C\" and \"D\" as answer',\n",
    "          'labels': labels,\n",
    "          'set':'target'\n",
    "\n",
    "}\n",
    "\n",
    "with open(f'{split}/{dataset_name}_test.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARC-Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/ai2_arc/1569c2591ea2683779581d9fb467203d9aa95543bb9b75dcfde5da92529fd7f6 (last modified on Fri Aug 11 17:17:10 2023) since it couldn't be found locally at ai2_arc., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1165\n"
     ]
    }
   ],
   "source": [
    "dataset_name='ARC-Challenge'\n",
    "dataset= load_dataset('ai2_arc',dataset_name)['test']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "import random\n",
    "\n",
    "label2text={\n",
    "    'A':'A',\n",
    "    'B':'B',\n",
    "    \"C\":\"C\",\n",
    "    \"D\":\"D\",\n",
    "    '1':'A',\n",
    "    '2':'B',\n",
    "    \"3\":\"C\",\n",
    "    \"4\":\"D\",\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "    if len(d['choices']['text'])!=4:\n",
    "        continue\n",
    "    test_id.append(d['id'])\n",
    "    id+=1\n",
    "    test_s.append(d['question']+'\\nA. '+d['choices']['text'][0]+'\\nB. '+d['choices']['text'][1]+'\\nC. '+d['choices']['text'][2]+'\\nD. '+d['choices']['text'][3])\n",
    "    test_l.append(label2text[d['answerKey']])\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Space: ['C', 'D', 'A', 'B']\n",
      "Count of selected labels: {'C': 125, 'D': 125, 'A': 125, 'B': 125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 2/2 [00:00<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_data, labels = create_new_dataset(test_s_df)\n",
    "prompt_temp_id='005'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)\n",
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'005',\n",
    "        'data':selected_data,\n",
    "        'instruction':'Given a question answering task from the 3rd to 9th-grade science exam. The question contains four options \"A.\", \"B.\", \"C.\" and \"D.\" Select the most appropriate choice that answers the question',\n",
    "        'labels': labels,\n",
    "        'set':'target'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_test.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_7175875',\n",
       " 'label': 'C',\n",
       " 'sentence': 'An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\\nA. Planetary density will decrease.\\nB. Planetary years will become longer.\\nC. Planetary days will become shorter.\\nD. Planetary gravity will become stronger.',\n",
       " 'emb': tensor([ 4.4570e-02,  2.2817e-02,  8.0172e-02, -3.3356e-02,  7.6208e-02,\n",
       "          1.4206e-02, -1.6502e-02,  3.1332e-02,  5.2658e-02,  3.3354e-02,\n",
       "          3.7082e-02,  3.7466e-02, -1.6066e-02, -1.7431e-02,  2.3428e-02,\n",
       "          1.5800e-02, -1.8905e-02,  4.8023e-02, -8.2188e-02,  5.7523e-02,\n",
       "         -3.9804e-02, -6.0829e-02, -5.9459e-02,  1.1388e-01,  6.3163e-02,\n",
       "          2.1221e-02, -4.0567e-03,  3.1933e-02, -8.7232e-03, -3.6615e-02,\n",
       "         -1.3328e-02,  3.0853e-02, -5.9815e-03, -1.5620e-02, -7.4276e-02,\n",
       "         -4.2818e-02, -1.4294e-02, -1.8323e-02, -7.9383e-02, -4.2442e-02,\n",
       "          1.5674e-03, -6.0584e-02,  7.6698e-02, -1.2377e-02, -5.2706e-02,\n",
       "         -1.3645e-02, -1.3243e-02, -4.2935e-04, -6.6594e-02,  4.4197e-02,\n",
       "         -7.4137e-02, -7.4116e-02, -9.2603e-02, -2.1939e-02, -3.8551e-02,\n",
       "         -1.0186e-02,  4.8533e-02, -7.0853e-03,  8.8457e-02, -8.2723e-02,\n",
       "         -2.1539e-02,  6.1028e-03, -1.4619e-02,  1.3417e-02, -1.2963e-02,\n",
       "         -5.3887e-02,  1.1741e-02, -5.6403e-02, -5.1667e-02,  5.7012e-02,\n",
       "          2.0335e-02,  7.3540e-02, -6.0231e-02, -6.9120e-02,  5.1907e-03,\n",
       "         -4.3620e-02,  3.5039e-02,  3.5706e-04,  4.2442e-02, -6.4441e-02,\n",
       "          8.8935e-02,  2.4515e-02, -2.4088e-02, -4.7816e-02, -3.4793e-02,\n",
       "         -3.7109e-04, -6.4261e-02,  4.2836e-02, -8.1284e-02,  8.5600e-03,\n",
       "          5.2375e-02,  3.6427e-02,  5.5574e-03,  1.1981e-02, -6.9045e-02,\n",
       "          8.8138e-02, -2.2781e-03, -1.0393e-01,  4.5916e-02,  8.0380e-03,\n",
       "         -1.0807e-02,  5.7901e-02, -1.0478e-01,  5.4431e-02, -4.7804e-02,\n",
       "          1.4466e-02, -5.4725e-02, -5.4045e-02, -5.3008e-02,  1.2915e-01,\n",
       "          5.6980e-02,  1.2756e-01, -4.3296e-02,  2.3231e-02, -6.2376e-02,\n",
       "          4.1528e-02, -7.9809e-03,  3.1129e-02, -1.1968e-01, -4.3920e-02,\n",
       "          7.4544e-03,  7.3778e-03, -2.4108e-02, -1.2702e-02,  1.4014e-02,\n",
       "         -1.7037e-02, -8.6152e-03, -1.7289e-34,  3.1706e-02, -7.2161e-02,\n",
       "         -3.1916e-02, -4.6513e-03,  3.4669e-02, -1.7918e-02, -3.6340e-02,\n",
       "          8.6787e-02,  4.0694e-02, -1.3336e-01, -8.7901e-02, -4.8443e-02,\n",
       "          3.0610e-02,  9.2156e-03, -4.5786e-02,  8.7531e-03,  5.1527e-02,\n",
       "          8.1658e-02, -2.8089e-02, -4.6011e-02, -8.5141e-02, -3.3575e-02,\n",
       "         -6.5217e-02,  9.0345e-02,  3.8054e-02, -6.6203e-02,  6.2521e-02,\n",
       "         -2.9743e-02, -1.1281e-01, -3.6660e-02,  3.3610e-02, -8.3155e-02,\n",
       "         -5.2528e-02,  2.0357e-02,  3.5860e-02,  1.3304e-02,  2.4145e-02,\n",
       "         -8.8403e-03, -1.7546e-02, -3.4294e-02, -1.0472e-01, -1.6493e-02,\n",
       "         -6.7842e-02,  2.3174e-02,  5.6816e-02, -4.1113e-03,  5.2265e-02,\n",
       "          1.8591e-02,  6.3679e-02, -3.5807e-03,  3.5599e-02,  3.3089e-02,\n",
       "          1.6720e-02,  5.1672e-02,  3.7717e-02,  6.6356e-02,  4.5124e-02,\n",
       "         -3.5244e-02, -3.6713e-02,  6.9433e-02,  4.7366e-02,  3.0604e-02,\n",
       "          1.1560e-01,  4.8534e-04,  7.8709e-02,  1.1691e-02,  8.0948e-02,\n",
       "         -2.3622e-02,  1.7790e-02,  2.3621e-03,  3.6217e-02,  1.0612e-02,\n",
       "          7.2534e-02,  6.3313e-02, -4.8224e-02,  1.0761e-01, -3.0556e-02,\n",
       "          3.6775e-02,  2.0508e-02,  1.0497e-02,  1.3930e-02,  4.9156e-03,\n",
       "         -1.0024e-02, -5.3415e-02, -1.7328e-02,  7.4011e-02, -1.0020e-03,\n",
       "         -2.5833e-02,  9.1302e-02,  9.0713e-03,  3.6720e-02, -9.5542e-02,\n",
       "          6.1883e-02,  1.7880e-02, -4.2870e-02, -2.0784e-33, -5.9223e-02,\n",
       "          4.9758e-04, -2.5629e-02,  9.3256e-02,  4.0286e-02, -5.4901e-02,\n",
       "         -4.6027e-02,  4.9735e-03, -9.0345e-02, -1.4112e-01,  5.2476e-02,\n",
       "          4.6615e-02, -1.9026e-02, -6.3965e-02,  8.2528e-02, -1.3205e-02,\n",
       "         -1.0246e-02, -4.1172e-02, -5.2545e-02,  9.2104e-02,  1.1212e-02,\n",
       "         -6.2570e-02,  6.1880e-02,  4.6702e-03,  7.1582e-03, -6.6667e-02,\n",
       "          2.6488e-02,  2.4356e-03,  3.8161e-02,  4.0839e-02, -3.7060e-02,\n",
       "          7.9880e-02,  1.6523e-02,  4.0569e-02,  1.3048e-02,  8.4032e-02,\n",
       "         -3.4573e-02, -9.3482e-02, -2.3442e-02,  1.4642e-02, -2.8126e-03,\n",
       "          4.8706e-02,  1.2296e-02, -2.7956e-02,  2.0951e-04, -6.9679e-02,\n",
       "          1.4835e-02,  1.1831e-01,  1.3499e-01,  7.8764e-02,  3.8422e-02,\n",
       "          9.7570e-03,  2.8145e-02,  3.3827e-02,  7.1824e-02,  5.0554e-02,\n",
       "          5.7664e-02, -8.2065e-02,  2.1435e-02,  2.6853e-02, -1.2005e-02,\n",
       "          3.8320e-03, -2.4255e-02, -9.7130e-03, -2.5397e-02, -8.3546e-02,\n",
       "         -3.2153e-02,  1.6075e-02,  1.1000e-02,  6.5966e-02,  5.7940e-03,\n",
       "          4.3962e-02,  2.3026e-02,  2.4975e-02, -3.4149e-02,  2.9209e-02,\n",
       "          1.2588e-01,  5.7302e-02, -3.7742e-02,  7.1622e-03, -9.1883e-02,\n",
       "          2.3194e-02,  1.3978e-02, -3.1475e-02, -1.1626e-01,  4.9012e-02,\n",
       "         -6.8594e-02, -1.3640e-01,  2.6721e-02,  2.0859e-03, -5.3693e-02,\n",
       "          1.4077e-02, -1.8130e-02, -5.8703e-02,  1.9068e-02, -3.4811e-08,\n",
       "          1.7651e-02, -8.6959e-03, -6.2004e-03,  1.2240e-02,  7.1405e-02,\n",
       "          2.3055e-02,  4.8153e-02,  8.3132e-02, -5.7014e-02, -5.1314e-02,\n",
       "          9.6927e-03,  2.8439e-02,  5.4284e-02, -2.3217e-02, -5.0665e-02,\n",
       "         -2.8933e-02, -1.6450e-02, -6.8051e-02, -1.7125e-02, -1.2591e-02,\n",
       "          7.4104e-02,  2.7639e-02,  2.8775e-02, -3.3905e-02, -2.5616e-02,\n",
       "          4.0826e-02, -1.7779e-02,  3.4113e-02, -1.3158e-01,  1.7928e-02,\n",
       "         -5.2714e-03, -2.9037e-02, -2.2102e-02, -6.3789e-02,  6.3593e-02,\n",
       "          3.5523e-02,  2.2750e-02, -9.6498e-03,  4.6819e-02,  5.0625e-02,\n",
       "         -1.3325e-02, -2.3640e-02, -5.0160e-02,  7.5601e-03, -5.8296e-02,\n",
       "         -9.3677e-02,  6.9695e-04, -7.0139e-02, -1.0532e-02, -1.2938e-02,\n",
       "         -1.8089e-02, -6.2101e-02,  7.5550e-03, -3.4188e-02,  2.8442e-02,\n",
       "          8.3083e-02,  1.0438e-02,  3.5204e-02,  1.7342e-02,  1.0376e-02,\n",
       "          6.0513e-02, -6.2679e-02, -3.1113e-02, -2.1760e-02])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social_i_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/social_i_qa/674d85e42ac7430d3dcd4de7007feaffcb1527c535121e09bab2803fbcc925f8 (last modified on Thu Sep 14 11:41:32 2023) since it couldn't be found locally at social_i_qa., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1954\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'social_i_qa'\n",
    "dataset= load_dataset(dataset_name)['validation']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "test_c=[]\n",
    "id=1\n",
    "\n",
    "text2label={\n",
    "    '1':'A',\n",
    "    '2':'B',\n",
    "    '3':'C'\n",
    "}\n",
    "label2text={\n",
    "    1:'answerA',\n",
    "    2:'answerB',\n",
    "    3:'answerC'\n",
    "}\n",
    "\n",
    "\n",
    "for d in dataset:\n",
    "          test_id.append(id)\n",
    "          id+=1\n",
    "\n",
    "          test_l.append(text2label[d['label']])\n",
    "          test_c.append(d['context'])\n",
    "\n",
    "          q=d['question']+' \\nA. '+d['answerA']+' \\nB. '+d['answerB']+' \\nC. '+d['answerC']\n",
    "          test_s.append(q)\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s,\n",
    "    'context':test_c\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Space: ['C', 'A', 'B']\n",
      "Count of selected labels: {'C': 167, 'A': 167, 'B': 167}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 2/2 [00:01<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_data, labels = create_new_dataset(test_s_df)\n",
    "\n",
    "prompt_temp_id='003'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)\n",
    "data_to_save={\n",
    "          'dataset_name':dataset_name,\n",
    "          'prompt_temp_id':'003',\n",
    "          'data': selected_data,\n",
    "          'instruction': 'Given an action as the context and a related question, you are to answer the question based on the context using your social intelligence. The question is of multiple choice form with three options \"A\", \"B\" and \"C\". Select the most appropriate answer from the provided choices \"A\", \"B\" and \"C\".',\n",
    "          'labels': labels,\n",
    "          'set':'target'\n",
    "          \n",
    "          }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_test.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial Pharasebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/financial_phrasebank/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141 (last modified on Fri Aug 11 17:37:50 2023) since it couldn't be found locally at financial_phrasebank., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2264\n"
     ]
    }
   ],
   "source": [
    "dataset_name='financial_phrasebank'\n",
    "dataset= load_dataset(dataset_name,'sentences_allagree')['train']\n",
    "dataset=dataset.train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
    "dataset=dataset['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "\n",
    "text2label={\n",
    "    1:'neutral',\n",
    "    2:'positive',\n",
    "    0:'negative'\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "          test_id.append(id)\n",
    "          id+=1\n",
    "          test_s.append(d['sentence'])\n",
    "          test_l.append(text2label[d['label']])\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Space: ['negative', 'positive', 'neutral']\n",
      "Count of selected labels: {'negative': 167, 'positive': 167, 'neutral': 167}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 3/3 [00:01<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_data, labels = create_new_dataset(test_s_df)\n",
    "\n",
    "prompt_temp_id='004'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)\n",
    "data_to_save={\n",
    "        'dataset_name': dataset_name,\n",
    "        'prompt_temp_id':'004',\n",
    "        'data': selected_data,\n",
    "        'instruction': 'Given a sentence mined from a financial news article, you are to determine the sentiment polarity of the sentence. The task deals with financial sentiment analysis. Based on the sentiment conveyed by the sentence, label the sentence as \"negative\", \"positive\" or \"neutral\"',\n",
    "        'labels': labels,\n",
    "        'set':'target'\n",
    "\n",
    "}\n",
    "\n",
    "with open(f'{split}/{dataset_name}_test.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_data[0]\n",
    "len(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tareget-unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=8\n",
    "split='data/target-unlabeled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataset(data_list, N = k):\n",
    "    \n",
    "    selected_samples = []\n",
    "\n",
    "    if type(data_list[0]['label']) == list:\n",
    "        labels = list(set([example['label'][0] for example in data_list]))\n",
    "\n",
    "        print(f\"Label Space: {labels}\")\n",
    "        count = {}\n",
    "        for label in labels:\n",
    "            count[label] = 0\n",
    "\n",
    "        np.random.seed(0)\n",
    "        chosen_indices = np.random.randint(low = 0, high = len(data_list), size = len(data_list))\n",
    "    \n",
    "        for chosen in chosen_indices:\n",
    "            if (chosen not in selected_samples) and (count[data_list[chosen]['label'][0]] < (N/(len(count)))):\n",
    "                selected_samples.append(chosen)\n",
    "                count[data_list[chosen]['label'][0]] +=1\n",
    "\n",
    "        selected_samples.sort()\n",
    "\n",
    "        print(f\"Count of selected labels: {count}\")\n",
    "\n",
    "        selected_data = list(np.array(data_list)[selected_samples])\n",
    "        return selected_data, labels\n",
    "\n",
    "    else:\n",
    "        labels = list(set([example['label'] for example in data_list]))\n",
    "\n",
    "        print(f\"Label Space: {labels}\")\n",
    "        count = {}\n",
    "        for label in labels:\n",
    "            count[label] = 0\n",
    "\n",
    "        np.random.seed(0)\n",
    "        chosen_indices = np.random.randint(low = 0, high = len(data_list), size = len(data_list))\n",
    "\n",
    "        for chosen in chosen_indices:\n",
    "\n",
    "            if (chosen not in selected_samples) and (count[data_list[chosen]['label']] < (N/(len(count)))):\n",
    "                selected_samples.append(chosen)\n",
    "                count[data_list[chosen]['label']] +=1\n",
    "\n",
    "\n",
    "        selected_samples.sort()\n",
    "        print(f\"Count of selected labels: {count}\")\n",
    "        selected_data = list(np.array(data_list)[selected_samples])\n",
    "        return selected_data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Med-mcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/medmcqa/f2fdfa9ccfbf9d148c0639e6afe3379f3c7e95c4d52d5e68ec1156e5004bd880 (last modified on Thu Sep 14 11:39:40 2023) since it couldn't be found locally at medmcqa., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182822\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'medmcqa'\n",
    "dataset= load_dataset(dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "\n",
    "text2label={\n",
    "    0:'A',\n",
    "    1:'B',\n",
    "    2:'C',\n",
    "    3:'D'\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "    test_id.append(d['id'])\n",
    "    test_l.append(text2label[d['cop']])\n",
    "\n",
    "    q=d['question']+' \\nA. '+d['opa']+' \\nB. '+d['opb']+' \\nC. '+d['opc']+' \\nD. '+d['opd']\n",
    "\n",
    "    test_s.append(q)\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Space: ['C', 'D', 'A', 'B']\n",
      "Count of selected labels: {'C': 2, 'D': 2, 'A': 2, 'B': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 1/1 [00:00<00:00, 45.61it/s]\n"
     ]
    }
   ],
   "source": [
    "selected_data, labels = create_new_dataset(test_s_df)\n",
    "prompt_temp_id='005'\n",
    "create_embeddings(selected_data,prompter,prompt_temp_id,model)\n",
    "data_to_save={\n",
    "          'dataset_name': dataset_name,\n",
    "          'prompt_temp_id':'005',\n",
    "          'data': selected_data,\n",
    "          'instruction': 'Given a multiple choice question containing four options \"A.\", \"B.\", \"C.\" and \"D.\" from a medical entrance exam. The question is related to a sub-field of medical science like Microbiology, Radiology, Ophthalmology, Surgery, Human anatomy, etc. Based on the question, the option and your knowledge of the medical field select the most appropriate answer from the provided choices \"A.\", \"B.\", \"C.\" and \"D.\".',\n",
    "          'labels': labels,\n",
    "          'set':'target'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sciq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/sciq/50e5c6e3795b55463819d399ec417bfd4c3c621105e00295ddb5f3633d708493 (last modified on Thu Sep 14 11:41:12 2023) since it couldn't be found locally at sciq., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11679\n",
      "Label Space: ['C', 'D', 'A', 'B']\n",
      "Count of selected labels: {'C': 2, 'D': 2, 'A': 2, 'B': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 1/1 [00:00<00:00, 49.17it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'sciq'\n",
    "dataset= load_dataset(dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "import random\n",
    "text2label={\n",
    "    'A':'distractor3',\n",
    "    'B':'distractor1',\n",
    "    'C':'distractor2',\n",
    "    'D':'correct_answer'\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "\n",
    "    k=['A','B','C','D']\n",
    "    k_=['A','B','C','D']\n",
    "    random.shuffle(k)\n",
    "    q=d['question']   \n",
    "    for i,l in zip(k,k_):\n",
    "        op=text2label[i]\n",
    "        if op=='correct_answer':\n",
    "            test_l.append(l)\n",
    "\n",
    "        q+=f'\\n{l}. '+d[op]\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "    test_id.append(id)\n",
    "    id+=1\n",
    "    test_s.append(q)\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))\n",
    "\n",
    "selected_data, labels = create_new_dataset(test_s_df)\n",
    "prompt_temp_id='005'\n",
    "create_embeddings(selected_data,prompter,prompt_temp_id,model)\n",
    "data_to_save={\n",
    "          'dataset_name': dataset_name,\n",
    "          'prompt_temp_id':'005',\n",
    "          'data': selected_data,\n",
    "          'instruction': 'Given a question from a scientific exam about Physics, Chemistry, and Biology, among others. The question is in multiple choice format with four answer options \"A.\", \"B.\", \"C.\" and \"D.\". Using your knowledge about the scientific fields answer the question and provide the label \"A\", \"B\", \"C\" and \"D\" as answer',\n",
    "          'labels': labels,\n",
    "          'set':'target'\n",
    "\n",
    "}\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARC-Challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/ai2_arc/1569c2591ea2683779581d9fb467203d9aa95543bb9b75dcfde5da92529fd7f6 (last modified on Fri Aug 11 17:17:10 2023) since it couldn't be found locally at ai2_arc., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1117\n",
      "Label Space: ['C', 'D', 'A', 'B']\n",
      "Count of selected labels: {'C': 2, 'D': 2, 'A': 2, 'B': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 2/2 [00:00<00:00,  2.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'Mercury_7216878',\n",
       " 'label': 'C',\n",
       " 'sentence': 'Some health care professionals recommend that children use insect repellent before going outside. Using insect repellent is a good way to keep the insects from\\nA. reproducing.\\nB. laying eggs.\\nC. spreading disease.\\nD. dying.',\n",
       " 'emb': tensor([-1.2950e-03,  4.3211e-02,  1.1702e-02,  3.7854e-02,  8.7574e-02,\n",
       "          1.0261e-01,  1.4885e-01, -1.2478e-02, -1.3461e-02,  4.7728e-02,\n",
       "          1.3910e-01,  3.8054e-02, -5.0591e-02,  8.3565e-02,  1.7855e-02,\n",
       "          1.2662e-01,  1.3615e-01,  2.2474e-02, -3.0715e-02, -9.3841e-02,\n",
       "         -2.4924e-02,  3.3524e-02,  4.3234e-02, -2.8324e-02, -4.4351e-02,\n",
       "          1.0436e-02, -3.8043e-02, -1.4960e-02,  4.3807e-02, -3.7751e-02,\n",
       "          2.7026e-02, -1.8748e-03, -9.1333e-02, -5.5277e-02, -6.3995e-02,\n",
       "          5.3490e-02,  2.3262e-03, -2.8691e-02, -4.1016e-02,  3.7887e-02,\n",
       "         -6.9167e-03,  5.6006e-02, -1.4653e-01, -3.0376e-02, -7.6202e-02,\n",
       "         -3.8039e-02,  6.8713e-02,  5.9085e-02,  6.2820e-02, -7.2662e-02,\n",
       "          1.2163e-01,  1.5181e-02, -1.2680e-02,  5.8828e-02,  1.4614e-02,\n",
       "         -7.0380e-02, -2.0071e-02, -5.4960e-02, -5.7715e-03, -2.9968e-02,\n",
       "         -1.1119e-02, -1.7024e-02, -2.3457e-02, -5.2077e-02, -4.6612e-02,\n",
       "         -6.0175e-02, -5.3490e-02,  5.6787e-02,  1.0978e-01,  3.7951e-02,\n",
       "         -3.5933e-02,  5.9608e-02, -2.0518e-03,  6.0519e-02, -3.7822e-02,\n",
       "          5.1148e-02, -6.8257e-03, -7.3782e-02,  1.0022e-01, -4.9118e-02,\n",
       "         -6.0695e-02,  1.4434e-02,  8.3535e-02,  2.5622e-02,  7.3220e-02,\n",
       "          3.1245e-02,  3.6382e-02,  5.9193e-03, -1.1062e-02, -2.0705e-02,\n",
       "          6.0002e-02, -1.8503e-02,  9.4284e-02,  3.3710e-02, -6.9700e-02,\n",
       "         -1.5067e-03, -7.7987e-02, -4.9218e-02, -8.1386e-02, -2.5974e-03,\n",
       "         -1.9478e-02, -6.1376e-02, -5.7116e-02,  2.7927e-02, -5.8761e-02,\n",
       "         -1.1321e-01, -1.3804e-02, -2.2611e-02,  3.3648e-03,  7.6013e-02,\n",
       "         -1.3429e-02, -2.0187e-03,  2.3631e-02,  9.2234e-02, -1.0373e-01,\n",
       "         -8.4349e-02, -1.5918e-02, -5.2533e-02,  4.7578e-02, -3.7620e-02,\n",
       "          3.3591e-02, -6.7086e-03,  4.0605e-02, -7.2354e-02,  9.0829e-02,\n",
       "         -1.2092e-01, -2.2507e-02,  1.3360e-33, -2.8513e-02,  3.6744e-02,\n",
       "          3.4626e-02,  2.3492e-02, -2.7557e-02,  1.6135e-03, -3.9182e-02,\n",
       "          1.8625e-02,  1.1060e-01, -3.1686e-02, -1.8698e-02, -1.0662e-01,\n",
       "          3.4179e-02,  2.0477e-02, -7.1665e-04,  7.4984e-02,  1.1218e-02,\n",
       "         -8.7827e-03,  1.9206e-02,  3.1333e-02, -6.1914e-02, -3.8134e-02,\n",
       "          1.4873e-02, -3.9422e-02,  6.3227e-02,  3.0265e-02, -4.3877e-02,\n",
       "          4.1354e-02, -4.0565e-02,  4.0696e-02,  1.1762e-01, -3.5921e-02,\n",
       "         -4.9605e-02,  1.0750e-01,  1.0194e-03,  6.8920e-02,  7.0836e-02,\n",
       "         -3.6398e-02, -6.2611e-02, -5.8329e-02, -4.7225e-02, -2.6892e-02,\n",
       "         -5.7486e-03,  7.7761e-03,  9.5826e-02, -4.7484e-02, -4.5105e-02,\n",
       "          4.6159e-03, -1.8713e-03, -6.0225e-02,  7.0907e-02,  8.5370e-02,\n",
       "          7.5290e-02, -5.9811e-02,  1.3087e-02,  3.0514e-02,  2.9521e-03,\n",
       "         -1.0260e-02, -3.5819e-02, -5.9619e-02,  1.1762e-02, -1.1018e-02,\n",
       "          1.0398e-02, -2.2930e-02,  2.9845e-02, -4.3001e-02,  1.5107e-02,\n",
       "          2.1030e-02, -6.8735e-02, -9.6542e-02,  5.0717e-02,  1.6386e-02,\n",
       "         -1.8268e-02,  6.6141e-03, -7.3418e-02, -1.4251e-02, -1.2772e-02,\n",
       "          5.9457e-02,  5.7600e-02, -2.0735e-02,  5.3674e-02, -7.1091e-02,\n",
       "         -2.6757e-02, -2.9459e-02, -4.8637e-02, -3.9881e-02,  2.7045e-02,\n",
       "          2.4716e-02,  6.0624e-02,  5.5354e-02,  7.1818e-02,  3.3488e-02,\n",
       "          4.5425e-03,  5.9804e-02, -2.0803e-02, -2.1908e-33, -2.9050e-02,\n",
       "         -2.1388e-02, -1.4424e-02, -5.6873e-02, -3.9242e-02,  2.8004e-02,\n",
       "         -5.0468e-02, -7.4472e-02, -1.8916e-02,  7.6944e-03, -1.0224e-01,\n",
       "          4.9837e-02, -5.8361e-02, -7.3698e-04, -1.6432e-02,  1.8148e-02,\n",
       "         -1.3027e-02, -2.9904e-02, -2.6180e-02, -8.2396e-02, -9.1454e-02,\n",
       "          1.6575e-02,  3.4575e-02, -1.1330e-02, -1.6302e-02, -1.7149e-02,\n",
       "          2.7920e-02,  4.0215e-02,  3.7498e-02, -2.7661e-02,  6.4898e-02,\n",
       "          1.8226e-03,  1.0883e-01, -3.8289e-02, -4.7692e-02, -3.6699e-02,\n",
       "          3.7829e-02, -1.8050e-02,  3.1083e-02,  9.5222e-03,  2.9164e-02,\n",
       "          1.8887e-02, -1.4302e-02, -5.2168e-02,  1.6860e-02,  3.8757e-02,\n",
       "         -6.6931e-02,  2.8254e-02,  3.7871e-02,  3.3402e-02,  2.5610e-02,\n",
       "          2.0410e-02, -6.7183e-02, -9.5375e-02, -9.0044e-03,  2.6330e-02,\n",
       "         -3.5818e-02, -3.5313e-02,  5.1125e-02, -1.0170e-02, -1.1899e-02,\n",
       "         -6.4631e-02, -8.4486e-03,  5.7330e-02, -4.5599e-02,  6.1078e-02,\n",
       "         -8.5697e-02,  1.5415e-01,  2.8589e-02, -4.0970e-02, -4.8076e-02,\n",
       "          7.3063e-02, -8.9027e-03, -4.5360e-02, -1.1272e-01, -2.8183e-02,\n",
       "          8.3432e-02, -1.4665e-02,  3.2530e-02,  4.6947e-02, -6.8842e-02,\n",
       "         -6.6265e-02, -4.4793e-02, -2.0510e-03,  1.8289e-02, -5.4515e-02,\n",
       "         -3.1778e-02, -8.2088e-02, -5.9157e-02, -2.2275e-02, -6.0354e-03,\n",
       "         -2.6864e-02, -4.3480e-02,  4.9811e-02,  1.0944e-02, -3.3987e-08,\n",
       "          5.1612e-02,  3.0549e-02,  7.8055e-02, -3.3542e-02, -7.8257e-03,\n",
       "         -1.7803e-02,  3.1155e-02, -4.0293e-03,  1.8519e-02,  6.1909e-02,\n",
       "         -4.4409e-02,  3.0334e-02,  7.0349e-02,  4.0741e-02,  5.8388e-02,\n",
       "         -1.5590e-02,  2.0403e-02, -7.6916e-02, -5.0525e-02, -1.0266e-02,\n",
       "          1.2373e-02,  6.6649e-02, -3.0165e-02,  4.9973e-02,  2.4357e-02,\n",
       "         -4.6946e-02,  9.3107e-02, -3.3825e-02, -1.7517e-02,  3.3962e-02,\n",
       "          2.8021e-03,  6.4090e-02, -1.5038e-02,  1.9001e-02, -3.9663e-02,\n",
       "         -1.9967e-02,  9.9789e-02,  1.8836e-02,  3.8863e-02, -5.8328e-03,\n",
       "          3.1613e-03,  6.7809e-03,  2.2315e-03, -2.0105e-02, -9.2034e-03,\n",
       "         -5.5679e-02,  9.3932e-04, -1.3162e-02, -7.9139e-02,  3.2697e-02,\n",
       "          1.8459e-03, -4.8145e-02,  3.3749e-02, -2.8528e-02, -2.5095e-02,\n",
       "         -3.3111e-02,  9.5129e-03, -5.3638e-02,  5.5970e-03,  4.7013e-02,\n",
       "          2.1308e-02,  2.7427e-03,  4.9413e-03,  7.6575e-02])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name='ARC-Challenge'\n",
    "dataset= load_dataset('ai2_arc',dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "import random\n",
    "\n",
    "label2text={\n",
    "    'A':'A',\n",
    "    'B':'B',\n",
    "    \"C\":\"C\",\n",
    "    \"D\":\"D\",\n",
    "    '2':'A',\n",
    "    '1':'B',\n",
    "    \"3\":\"C\",\n",
    "    \"4\":\"D\",\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "    if len(d['choices']['text'])!=4:\n",
    "        continue\n",
    "    test_id.append(d['id'])\n",
    "    id+=1\n",
    "    test_s.append(d['question']+'\\nA. '+d['choices']['text'][0]+'\\nB. '+d['choices']['text'][1]+'\\nC. '+d['choices']['text'][2]+'\\nD. '+d['choices']['text'][3])\n",
    "    test_l.append(label2text[d['answerKey']])\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))\n",
    "selected_data, labels = create_new_dataset(test_s_df)\n",
    "prompt_temp_id='005'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)\n",
    "data_to_save={\n",
    "        'dataset_name':dataset_name,\n",
    "        'prompt_temp_id':'005',\n",
    "        'data':selected_data,\n",
    "        'instruction':'Given a question answering task from the 3rd to 9th-grade science exam. The question contains four options \"A.\", \"B.\", \"C.\" and \"D.\" Select the most appropriate choice that answers the question',\n",
    "        'labels': labels,\n",
    "        'set':'target'\n",
    "        }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social_i_qa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/social_i_qa/674d85e42ac7430d3dcd4de7007feaffcb1527c535121e09bab2803fbcc925f8 (last modified on Thu Sep 14 11:41:32 2023) since it couldn't be found locally at social_i_qa., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33410\n",
      "Label Space: ['C', 'A', 'B']\n",
      "Count of selected labels: {'C': 3, 'A': 3, 'B': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 33/33 [00:19<00:00,  1.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 2733,\n",
       " 'label': 'A',\n",
       " 'sentence': \"What does Robin need to do before this? \\nA. make sure Riley doesn't fall off the ladder \\nB. be prepared for them to fall \\nC. safe\",\n",
       " 'context': \"Robin supported Riley's weight to help hold them steady on the ladder.\",\n",
       " 'emb': tensor([-6.4449e-02,  1.7841e-02,  3.6968e-02,  1.9502e-02,  1.4299e-02,\n",
       "          5.4664e-02,  3.8244e-02,  1.1291e-01, -7.5707e-02,  4.5942e-02,\n",
       "          3.1504e-02, -3.5769e-02, -2.0830e-02,  2.1833e-02, -4.6969e-02,\n",
       "          7.0748e-02, -1.7893e-02,  5.3615e-02, -2.9572e-02,  2.7360e-02,\n",
       "         -2.9490e-02, -1.0689e-01,  3.9402e-02,  9.0853e-02, -5.0609e-02,\n",
       "         -1.5155e-02, -7.8933e-02, -1.3846e-02,  3.3759e-02, -1.3335e-02,\n",
       "          7.6841e-03, -4.0268e-02,  1.8667e-02,  2.1241e-02, -4.5002e-02,\n",
       "          4.7769e-02,  4.2563e-02,  1.8124e-02, -3.0843e-02, -8.2606e-03,\n",
       "          7.7016e-03, -1.8081e-02, -4.8170e-02,  2.8050e-03,  2.6291e-02,\n",
       "          5.8090e-02, -2.9885e-02,  5.0821e-02,  2.7274e-02,  2.8839e-02,\n",
       "         -6.5403e-03,  2.2565e-02, -4.2525e-02, -3.5677e-02, -4.7990e-03,\n",
       "          1.1099e-01,  4.3838e-02, -6.2978e-02,  9.3323e-02,  4.5307e-02,\n",
       "          4.3369e-02,  3.0731e-02, -4.9632e-02,  5.0531e-02,  5.1960e-02,\n",
       "          4.0694e-03, -4.0863e-02,  1.8915e-02, -1.1085e-02,  1.3583e-01,\n",
       "          5.2569e-02, -6.5474e-02, -2.8376e-02, -6.5894e-03, -3.6681e-02,\n",
       "         -3.0325e-02, -8.3887e-03, -8.8466e-02,  5.3723e-02,  5.3895e-02,\n",
       "         -6.4700e-02,  1.4026e-02,  8.5442e-03,  4.9373e-02, -6.0796e-02,\n",
       "          3.8608e-02,  4.8018e-02, -2.4387e-02, -1.3676e-02, -2.0294e-02,\n",
       "         -4.3427e-03, -5.6267e-02, -2.9397e-02,  5.5076e-02,  7.4529e-02,\n",
       "          8.5265e-02, -1.5751e-01, -7.1700e-02, -6.0980e-02,  1.7625e-02,\n",
       "         -2.5544e-02,  2.8748e-02,  2.6270e-02,  1.3198e-02, -9.3960e-02,\n",
       "         -1.0605e-01,  3.3173e-02, -1.2903e-02,  7.6201e-02,  6.5847e-02,\n",
       "          4.6297e-02, -6.4708e-02,  3.6739e-02,  8.3648e-02, -7.4212e-02,\n",
       "          2.7294e-02,  3.4062e-02, -9.1635e-02, -6.3498e-02,  1.4736e-02,\n",
       "          6.4094e-02,  7.2585e-02,  5.4716e-02, -9.5139e-03, -7.0231e-02,\n",
       "         -4.4096e-02, -1.7102e-02, -1.8846e-34, -3.5323e-02, -3.1264e-02,\n",
       "          5.5180e-02,  2.5837e-02,  5.0897e-02, -1.9718e-02, -5.3734e-02,\n",
       "         -8.8203e-02, -3.2359e-02,  2.9824e-02, -3.7820e-02, -3.2386e-02,\n",
       "         -8.3170e-03, -5.9988e-02, -3.5429e-02, -1.6139e-03,  6.5888e-02,\n",
       "          1.4411e-04, -9.9389e-03, -5.9971e-02,  6.6706e-02, -5.8573e-02,\n",
       "         -2.5524e-02,  4.6195e-02,  3.8634e-02,  4.2088e-03, -1.9609e-02,\n",
       "         -4.1637e-02, -3.1520e-03,  1.0294e-02, -5.3111e-02,  8.5621e-03,\n",
       "         -6.1078e-02, -9.5187e-02, -1.1016e-02,  3.8922e-02, -7.0117e-03,\n",
       "         -6.6064e-02, -1.7845e-02, -4.0861e-02, -2.3921e-02,  1.9244e-02,\n",
       "          6.9949e-02,  4.1191e-02,  1.3931e-02, -4.4628e-02,  7.0734e-02,\n",
       "         -2.5553e-02, -7.3031e-02,  4.6598e-02,  3.1291e-02, -2.1574e-02,\n",
       "          1.5312e-02, -6.6195e-02,  6.4278e-02,  6.7110e-02,  2.3520e-02,\n",
       "         -3.7819e-03, -7.0819e-03,  8.6615e-04,  4.5212e-02, -1.0462e-01,\n",
       "         -8.4426e-02, -2.3000e-02, -2.9158e-02, -4.9354e-02, -1.8838e-02,\n",
       "         -2.4183e-02, -4.7541e-02, -1.1027e-01, -1.0939e-01, -3.9000e-02,\n",
       "         -1.2353e-02,  5.6432e-02, -5.5035e-02,  6.7997e-02, -3.9937e-02,\n",
       "          3.3435e-02,  1.0879e-01, -1.1210e-01,  1.6810e-02, -1.0558e-02,\n",
       "         -3.1386e-02,  3.1601e-02,  4.3743e-02, -7.7420e-02, -9.5874e-03,\n",
       "          3.2099e-03, -3.0125e-02,  7.6714e-02, -3.8321e-03, -1.5895e-02,\n",
       "         -3.1813e-02,  3.3669e-02,  2.1572e-02, -2.4065e-33, -2.2994e-02,\n",
       "          5.3753e-02, -8.3498e-02, -2.4751e-02,  5.6852e-02, -1.0040e-02,\n",
       "         -2.4311e-02, -2.0548e-01,  1.2040e-01, -3.3438e-02, -6.7031e-02,\n",
       "         -4.0392e-02, -4.3923e-02,  3.3527e-02,  3.9497e-02, -1.6994e-02,\n",
       "          2.3680e-02,  1.2593e-02,  2.0841e-02,  4.8530e-02,  6.6581e-02,\n",
       "         -1.1552e-02, -4.6081e-02,  1.3891e-01,  5.5419e-02,  5.6172e-02,\n",
       "          3.2050e-02, -9.8015e-03, -3.6522e-02, -5.9363e-03, -2.5360e-02,\n",
       "         -7.4014e-02,  1.5497e-02, -3.9226e-02, -3.3328e-04,  2.4192e-02,\n",
       "          3.5288e-02, -2.6324e-03, -1.5259e-02,  4.8285e-03,  1.2360e-01,\n",
       "          6.5708e-02,  2.6909e-02, -5.2941e-02,  4.7659e-02, -2.2792e-02,\n",
       "          1.0215e-01, -3.7737e-02, -8.1797e-02, -5.1505e-02, -3.5173e-02,\n",
       "         -9.2866e-02, -3.7220e-03,  4.3249e-02, -6.6709e-02, -2.2661e-02,\n",
       "         -3.4202e-03,  2.7535e-02, -1.6516e-02, -5.6906e-02,  1.4244e-03,\n",
       "          8.7626e-03, -2.0278e-02,  5.3475e-02, -5.9802e-02, -1.9748e-02,\n",
       "         -9.5146e-02,  5.7835e-03, -2.5977e-02,  7.3796e-04, -4.3717e-02,\n",
       "          3.0146e-02,  5.1999e-02, -3.6427e-02,  3.3437e-03,  6.0669e-03,\n",
       "         -2.8532e-02, -4.2920e-02,  2.5767e-02, -8.2630e-02, -1.0548e-01,\n",
       "          3.4312e-02, -1.1813e-02,  7.2316e-02,  7.4426e-03,  2.7634e-02,\n",
       "          3.5179e-02,  2.8083e-02,  5.0304e-02, -6.6389e-03, -3.0353e-02,\n",
       "         -9.4216e-03,  4.1615e-02, -7.4383e-03, -4.2382e-02, -3.5329e-08,\n",
       "         -3.5959e-02,  5.6409e-02,  2.5568e-02,  2.5541e-03,  3.2212e-02,\n",
       "          3.5947e-02,  1.5257e-02, -3.0533e-02,  5.6262e-03,  8.7190e-03,\n",
       "          2.9509e-02,  5.8335e-02,  6.1816e-02,  3.8897e-02, -1.9359e-02,\n",
       "          2.9783e-02, -9.1380e-02, -9.0559e-03, -5.4045e-02,  2.8813e-02,\n",
       "         -4.9338e-02, -2.9858e-02,  3.5788e-02,  9.1892e-02, -4.6997e-04,\n",
       "         -7.8073e-02,  4.6590e-02,  4.0351e-02, -4.0842e-02,  1.2478e-01,\n",
       "          4.5538e-02,  8.6386e-02, -8.0834e-02, -3.5714e-02,  1.6929e-03,\n",
       "          3.7395e-02,  1.0827e-01,  8.4239e-03,  4.8014e-02,  3.7801e-02,\n",
       "         -4.9629e-02,  9.0349e-02,  1.4410e-02, -4.3010e-03,  3.7703e-02,\n",
       "          3.3182e-02, -6.1863e-02,  2.4477e-02,  1.6226e-02, -8.7073e-02,\n",
       "         -7.9356e-03, -4.6769e-02, -6.5379e-02,  1.2376e-01,  3.6397e-02,\n",
       "         -6.5277e-02,  1.5248e-02, -1.0215e-02,  3.2037e-02, -4.6308e-02,\n",
       "         -4.8285e-02, -6.4918e-03,  4.3118e-02, -2.0397e-02])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'social_i_qa'\n",
    "dataset= load_dataset(dataset_name)['train']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "test_c=[]\n",
    "id=1\n",
    "\n",
    "text2label={\n",
    "    '1':'A',\n",
    "    '2':'B',\n",
    "    '3':'C'\n",
    "}\n",
    "label2text={\n",
    "    1:'answerA',\n",
    "    2:'answerB',\n",
    "    3:'answerC'\n",
    "}\n",
    "\n",
    "\n",
    "for d in dataset:\n",
    "          test_id.append(id)\n",
    "          id+=1\n",
    "\n",
    "          test_l.append(text2label[d['label']])\n",
    "          test_c.append(d['context'])\n",
    "\n",
    "          q=d['question']+' \\nA. '+d['answerA']+' \\nB. '+d['answerB']+' \\nC. '+d['answerC']\n",
    "          test_s.append(q)\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s,\n",
    "    'context':test_c\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))\n",
    "selected_data, labels = create_new_dataset(test_s_df)\n",
    "\n",
    "prompt_temp_id='003'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)\n",
    "data_to_save={\n",
    "          'dataset_name':dataset_name,\n",
    "          'prompt_temp_id':'003',\n",
    "          'data': selected_data,\n",
    "          'instruction': 'Given an action as the context and a related question, you are to answer the question based on the context using your social intelligence. The question is of multiple choice form with three options \"A\", \"B\" and \"C\". Select the most appropriate answer from the provided choices \"A\", \"B\" and \"C\".',\n",
    "          'labels': labels,\n",
    "          'set':'target'\n",
    "          \n",
    "          }\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial Pharasebank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/eshaan/.cache/huggingface/modules/datasets_modules/datasets/financial_phrasebank/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141 (last modified on Fri Aug 11 17:37:50 2023) since it couldn't be found locally at financial_phrasebank., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2264\n",
      "Label Space: ['negative', 'positive', 'neutral']\n",
      "Count of selected labels: {'negative': 3, 'positive': 3, 'neutral': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting emb of batches: 100%|██████████| 3/3 [00:01<00:00,  2.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name='financial_phrasebank'\n",
    "dataset= load_dataset(dataset_name,'sentences_allagree')['train']\n",
    "dataset=dataset.train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
    "dataset=dataset['test']\n",
    "test_id=[]\n",
    "test_s=[]\n",
    "test_l=[]\n",
    "id=1\n",
    "\n",
    "text2label={\n",
    "    1:'neutral',\n",
    "    2:'positive',\n",
    "    0:'negative'\n",
    "}\n",
    "\n",
    "for d in dataset:\n",
    "          test_id.append(id)\n",
    "          id+=1\n",
    "          test_s.append(d['sentence'])\n",
    "          test_l.append(text2label[d['label']])\n",
    "\n",
    "df={\n",
    "    'id':test_id,\n",
    "    'label':test_l,\n",
    "    'sentence':test_s\n",
    "}\n",
    "test_df=pd.DataFrame(df)\n",
    "test_s_df=test_df.to_dict('records')\n",
    "print(len(test_s_df))\n",
    "selected_data, labels = create_new_dataset(test_s_df)\n",
    "\n",
    "prompt_temp_id='004'\n",
    "create_embeddings(test_s_df,prompter,prompt_temp_id,model)\n",
    "data_to_save={\n",
    "        'dataset_name': dataset_name,\n",
    "        'prompt_temp_id':'004',\n",
    "        'data': selected_data,\n",
    "        'instruction': 'Given a sentence mined from a financial news article, you are to determine the sentiment polarity of the sentence. The task deals with financial sentiment analysis. Based on the sentiment conveyed by the sentence, label the sentence as \"negative\", \"positive\" or \"neutral\"',\n",
    "        'labels': labels,\n",
    "        'set':'target'\n",
    "\n",
    "}\n",
    "\n",
    "with open(f'{split}/{dataset_name}_train.pkl', 'wb') as openfile:\n",
    "        # Reading from json file\n",
    "        pickle.dump(data_to_save, openfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
