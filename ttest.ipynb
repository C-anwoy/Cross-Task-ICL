{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "import pandas as pd\n",
    "import os\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_method = 'zero-shot'\n",
    "our_method = 'totally-cross-sim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_TASKS = ['ARC-Easy', 'ag_news', 'boolq', 'commonsense_qa', 'conll2003_pos', 'conll2003_ner', 'mnli', 'qqp', 'race', 'sst2']\n",
    "TARGET_TASKS = ['ARC-Challenge', 'financial_phrasebank', 'medmcqa', 'sciq', 'social_i_qa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'acc-results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path='results/dataset_name={}/sourcce_dataset={}-model={}-method={}-shots={}.csv'\n",
    "def eval(dataset_name,source_dataset,model='kglm_text_davinci_003',method='totally-cross-sim',k=1):\n",
    "    r=[]\n",
    "    df=pd.read_csv(result_path.format(dataset_name,source_dataset,model,method,k))\n",
    "    df=df.fillna('')\n",
    "    output=list(df['pred'])\n",
    "    gold=np.array(df['true_label'])\n",
    "    gold=[str(o).lower() for o in gold]\n",
    "    \n",
    "    output=[str(o).replace('Label:','') for o in output]\n",
    "    output = [o.strip(' .[]\":\\'').lower() for o in output]\n",
    "    output = [o.split('.')[0] for o in output]\n",
    "    output = [o.split(',')[0] for o in output]\n",
    "    output = [o.split(':')[0] for o in output]\n",
    "    output = [o.split('-')[0] for o in output]\n",
    "    \n",
    "    output = np.array(output)\n",
    "    acc=np.mean(output==gold)*100\n",
    "    print(f\"Acc for {dataset_name} using {source_dataset} is \",acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c2', 'd', 'e'], dtype='<U32')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = np.array([])\n",
    "arr2 = np.array(['c2', 'd', 'e'])\n",
    "arr3 = np.concatenate((arr1, arr2))\n",
    "arr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = {'llama_7b': '_home_models_savedModels_Llama-2-7b-hf_', 'llama_13b': '_home_eshaan_models_Llama-2-13b-hf_', 'gpt': 'kglm_text_davinci_003'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_output(path):\n",
    "    r=[]\n",
    "    df=pd.read_csv(path)\n",
    "    df=df.fillna('')\n",
    "    output=list(df['pred'])\n",
    "    gold=np.array(df['true_label'])\n",
    "    gold=[str(o).lower() for o in gold]\n",
    "    \n",
    "    output=[str(o).replace('Label:','') for o in output]\n",
    "    output = [o.strip(' .[]\":\\'').lower() for o in output]\n",
    "    output = [o.split('.')[0] for o in output]\n",
    "    output = [o.split(',')[0] for o in output]\n",
    "    output = [o.split(':')[0] for o in output]\n",
    "    output = [o.split('-')[0] for o in output]\n",
    "    \n",
    "    output = np.array(output)\n",
    "    gold = np.array(gold)\n",
    "    correct = (output==gold)\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_t_value(model, model_path):\n",
    "\n",
    "    with open(f'{model}_one_tail_t_test.txt', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            print(f'############## {model} ##############')\n",
    "    \n",
    "    for source_task in SOURCE_TASKS:\n",
    "    \n",
    "        baseline_predictions = np.array([])\n",
    "        our_predictions = np.array([])\n",
    "\n",
    "        for target_task in TARGET_TASKS:\n",
    "            if model == 'gpt':\n",
    "                baseline_result_path=f'acc-results/dataset_name={target_task}/sourcce_dataset=None-model={model_path[model]}-method={baseline_method}-shots={1}.csv'\n",
    "            else:\n",
    "                baseline_result_path=f'acc-results/dataset_name={target_task}/sourcce_dataset=None-model={model_path[model]}-method={baseline_method}-shots={0}.csv'\n",
    "\n",
    "            our_result_path=f'acc-results/dataset_name={target_task}/sourcce_dataset={source_task}-model={model_path[model]}-method={our_method}-shots={1}.csv'\n",
    "\n",
    "            baseline_acc = get_processed_output(baseline_result_path)\n",
    "            our_acc = get_processed_output(our_result_path) \n",
    "\n",
    "            baseline_predictions = np.concatenate((baseline_predictions, baseline_acc))\n",
    "            our_predictions = np.concatenate((our_predictions, our_acc))\n",
    "        \n",
    "        # print(f\"Baseline Predictions: {baseline_predictions.shape}\")\n",
    "        # print(f\"Our Predictions: {our_predictions.shape}\")\n",
    "\n",
    "        # Perform a one-tail paired t-test\n",
    "        t_statistic, p_value = ttest_rel(our_predictions, baseline_predictions)\n",
    "\n",
    "        # Set the significance level\n",
    "        alpha = 0.05\n",
    "\n",
    "        one_tail_p_value = (p_value / 2)\n",
    "        # Check if the p-value is less than the significance level\n",
    "        if one_tail_p_value < alpha and t_statistic > 0:\n",
    "            with open(f'{model}_one_tail_t_test.txt', 'a') as f:\n",
    "                with redirect_stdout(f):\n",
    "                    print(f'{source_task} => p-value: {one_tail_p_value:.2e}, t-statistic: {t_statistic:.4f}) => SIGNIFICANT')\n",
    "\n",
    "        else:\n",
    "            with open(f'{model}_one_tail_t_test.txt', 'a') as f:\n",
    "                with redirect_stdout(f):\n",
    "                    print(f'{source_task} => p-value: {one_tail_p_value:.2e}, t-statistic: {t_statistic:.4f}) => NOT SIGNIFICANT')\n",
    "        \n",
    "        print(f\"Done with {source_task} !\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ llama_7b ##################\n",
      "Done with ARC-Easy !\n",
      "Done with ag_news !\n",
      "Done with boolq !\n",
      "Done with commonsense_qa !\n",
      "Done with conll2003_pos !\n",
      "Done with conll2003_ner !\n",
      "Done with mnli !\n",
      "Done with qqp !\n",
      "Done with race !\n",
      "Done with sst2 !\n",
      "################ llama_13b ##################\n",
      "Done with ARC-Easy !\n",
      "Done with ag_news !\n",
      "Done with boolq !\n",
      "Done with commonsense_qa !\n",
      "Done with conll2003_pos !\n",
      "Done with conll2003_ner !\n",
      "Done with mnli !\n",
      "Done with qqp !\n",
      "Done with race !\n",
      "Done with sst2 !\n",
      "################ gpt ##################\n",
      "Done with ARC-Easy !\n",
      "Done with ag_news !\n",
      "Done with boolq !\n",
      "Done with commonsense_qa !\n",
      "Done with conll2003_pos !\n",
      "Done with conll2003_ner !\n",
      "Done with mnli !\n",
      "Done with qqp !\n",
      "Done with race !\n",
      "Done with sst2 !\n"
     ]
    }
   ],
   "source": [
    "for key in model_path.keys():\n",
    "    print(f\"################ {key} ##################\")\n",
    "    get_p_t_value(key, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
